{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5514e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from simple_linear_regr_utils import generate_data, evaluate\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "913ba760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "    \"\"\"\n",
    "    Generates a random dataset from a normal distribution.\n",
    "\n",
    "    Returns:\n",
    "        diabetes_X_train: the training dataset\n",
    "        diabetes_y_train: The output corresponding to the training set\n",
    "        diabetes_X_test: the test dataset\n",
    "        diabetes_y_test: The output corresponding to the test set\n",
    "\n",
    "    \"\"\"\n",
    "    # Load the diabetes dataset\n",
    "    diabetes_X, diabetes_y = load_diabetes(return_X_y=True)\n",
    "\n",
    "    # Use only one feature\n",
    "    diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "    # Split the data into training/testing sets\n",
    "    diabetes_X_train = diabetes_X[:-20]\n",
    "    diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "    # Split the targets into training/testing sets\n",
    "    diabetes_y_train = diabetes_y[:-20].reshape(-1,1)\n",
    "    diabetes_y_test = diabetes_y[-20:].reshape(-1,1)\n",
    "\n",
    "    print(f\"# Training Samples: {len(diabetes_X_train)}; # Test samples: {len(diabetes_X_test)};\")\n",
    "    return diabetes_X_train, diabetes_y_train, diabetes_X_test, diabetes_y_test\n",
    "\n",
    "\n",
    "def evaluate(model, X, y, y_predicted):\n",
    "    \"\"\" Calculates and prints evaluation metrics. \"\"\"\n",
    "    # The coefficients\n",
    "    print(f\"Slope: {model.W}; Intercept: {model.b}\")\n",
    "    # The mean squared error\n",
    "    mse = mean_squared_error(y, y_predicted)\n",
    "    print(f\"Mean squared error: {mse:.2f}\")\n",
    "    # The coefficient of determination: 1 is perfect prediction\n",
    "    r2 = r2_score(y, y_predicted)\n",
    "    print(f\"Coefficient of determination: {r2:.2f}\")\n",
    "\n",
    "    # Plot outputs\n",
    "    plt.scatter(X, y, color=\"black\")\n",
    "    plt.plot(X, y_predicted, color=\"blue\", linewidth=3)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if r2 >= 0.4:\n",
    "        print(\"****** Success ******\")\n",
    "    else:\n",
    "        print(\"****** Failed ******\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4dca080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegression:\n",
    "    def __init__(self, iterations=15000, lr=0.1):\n",
    "        self.iterations = iterations # number of iterations the fit method will be called\n",
    "        self.lr = lr # The learning rate\n",
    "        self.losses = [] # A list to hold the history of the calculated losses\n",
    "        self.W, self.b = None, None # the slope and the intercept of the model\n",
    "\n",
    "    def __loss(self, y, y_hat):\n",
    "        \"\"\"\n",
    "\n",
    "        :param y: the actual output on the training set\n",
    "        :param y_hat: the predicted output on the training set\n",
    "        :return:\n",
    "            loss: the sum of squared error\n",
    "\n",
    "        \"\"\"\n",
    "        #ToDO calculate the loss. use the sum of squared error formula for simplicity\n",
    "        loss = np.sum((y - y_hat)**2)\n",
    "\n",
    "        self.losses.append(loss)\n",
    "        return loss\n",
    "\n",
    "    def __init_weights(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        :param X: The training set\n",
    "        \"\"\"\n",
    "        weights = np.random.normal(size=X.shape[1] + 1)\n",
    "        self.W = weights[:X.shape[1]]#.reshape(-1, X.shape[1])\n",
    "        self.b = weights[-1]\n",
    "\n",
    "    def __sgd(self, X, y, y_hat):\n",
    "        \"\"\"\n",
    "\n",
    "        :param X: The training set\n",
    "        :param y: The actual output on the training set\n",
    "        :param y_hat: The predicted output on the training set\n",
    "        :return:\n",
    "            sets updated W and b to the instance Object (self)\n",
    "        \"\"\"\n",
    "        # ToDo calculate dW & db.\n",
    "        dW = (2/len(y)) * np.sum(X*(y_hat-y))\n",
    "        db = (2/len(y)) * np.sum((y_hat-y))\n",
    "        #print('n = ',len(y))\n",
    "        #print('dW ',dW)\n",
    "        #print('db ',db)\n",
    "        #  ToDO update the self.W and self.b using the learning rate and the values for dW and db\n",
    "        self.W = self.W - (self.lr * dW)\n",
    "        self.b = self.b - (self.lr * db)\n",
    "        #print(pd.DataFrame({'W':self.W, 'b':self.b}))\n",
    "        \n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "\n",
    "        :param X: The training set\n",
    "        :param y: The true output of the training set\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.__init_weights(X)\n",
    "        y_hat = self.predict(X)\n",
    "        #print('ndim =',y_hat)\n",
    "        #print(pd.DataFrame({'Actual':y.flatten(), 'predicted':y_hat.flatten()}))\n",
    "        loss = self.__loss(y, y_hat)\n",
    "        #print(f\"Initial Loss: {loss}\")\n",
    "        for i in range(self.iterations + 1):\n",
    "            self.__sgd(X, y, y_hat)\n",
    "            y_hat = self.predict(X)\n",
    "            loss = self.__loss(y, y_hat)\n",
    "            #print('iteration = ',i)\n",
    "            #print(pd.DataFrame({'Actual':y.flatten(), 'predicted':y_hat.flatten()}))\n",
    "            if not i % 100:\n",
    "                print(f\"Iteration {i}, Loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        :param X: The training dataset\n",
    "        :return:\n",
    "            y_hat: the predicted output\n",
    "        \"\"\"\n",
    "        #ToDO calculate the predicted output y_hat. remember the function of a line is defined as y = WX + b\n",
    "        y_hat = (self.W * X) + self.b\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "515a6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "63302f85",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training Samples: 422; # Test samples: 20;\n",
      "Iteration 0, Loss: 8929229.69069947\n",
      "Iteration 100, Loss: 2435124.810848781\n",
      "Iteration 200, Loss: 2368762.3159354315\n",
      "Iteration 300, Loss: 2308148.3443904687\n",
      "Iteration 400, Loss: 2252784.9414435057\n",
      "Iteration 500, Loss: 2202217.286697535\n",
      "Iteration 600, Loss: 2156029.9576968704\n",
      "Iteration 700, Loss: 2113843.517156327\n",
      "Iteration 800, Loss: 2075311.3958151003\n",
      "Iteration 900, Loss: 2040117.0453074258\n",
      "Iteration 1000, Loss: 2007971.337660342\n",
      "Iteration 1100, Loss: 1978610.190054948\n",
      "Iteration 1200, Loss: 1951792.3953381495\n",
      "Iteration 1300, Loss: 1927297.6404621438\n",
      "Iteration 1400, Loss: 1904924.6965727722\n",
      "Iteration 1500, Loss: 1884489.7658779868\n",
      "Iteration 1600, Loss: 1865824.9717156459\n",
      "Iteration 1700, Loss: 1848776.9794162922\n",
      "Iteration 1800, Loss: 1833205.7366310256\n",
      "Iteration 1900, Loss: 1818983.3227760582\n",
      "Iteration 2000, Loss: 1805992.8981419127\n",
      "Iteration 2100, Loss: 1794127.744034021\n",
      "Iteration 2200, Loss: 1783290.3860592884\n",
      "Iteration 2300, Loss: 1773391.7933562757\n",
      "Iteration 2400, Loss: 1764350.6471905229\n",
      "Iteration 2500, Loss: 1756092.6729063985\n",
      "Iteration 2600, Loss: 1748550.0297473292\n",
      "Iteration 2700, Loss: 1741660.7535316807\n",
      "Iteration 2800, Loss: 1735368.2476057662\n",
      "Iteration 2900, Loss: 1729620.8178920716\n",
      "Iteration 3000, Loss: 1724371.2482130318\n",
      "Iteration 3100, Loss: 1719576.4124015681\n",
      "Iteration 3200, Loss: 1715196.920011803\n",
      "Iteration 3300, Loss: 1711196.7927194037\n",
      "Iteration 3400, Loss: 1707543.168753121\n",
      "Iteration 3500, Loss: 1704206.032929379\n",
      "Iteration 3600, Loss: 1701157.9700720962\n",
      "Iteration 3700, Loss: 1698373.939792044\n",
      "Iteration 3800, Loss: 1695831.0707755047\n",
      "Iteration 3900, Loss: 1693508.4728922825\n",
      "Iteration 4000, Loss: 1691387.065579495\n",
      "Iteration 4100, Loss: 1689449.4210912865\n",
      "Iteration 4200, Loss: 1687679.6213267383\n",
      "Iteration 4300, Loss: 1686063.1270597777\n",
      "Iteration 4400, Loss: 1684586.658496803\n",
      "Iteration 4500, Loss: 1683238.0861807673\n",
      "Iteration 4600, Loss: 1682006.3313454916\n",
      "Iteration 4700, Loss: 1680881.274901588\n",
      "Iteration 4800, Loss: 1679853.6743063072\n",
      "Iteration 4900, Loss: 1678915.087634374\n",
      "Iteration 5000, Loss: 1678057.8042260369\n",
      "Iteration 5100, Loss: 1677274.7813426033\n",
      "Iteration 5200, Loss: 1676559.586309061\n",
      "Iteration 5300, Loss: 1675906.3436684876\n",
      "Iteration 5400, Loss: 1675309.6869141061\n",
      "Iteration 5500, Loss: 1674764.7144024575\n",
      "Iteration 5600, Loss: 1674266.9490855117\n",
      "Iteration 5700, Loss: 1673812.3017309054\n",
      "Iteration 5800, Loss: 1673397.0373281569\n",
      "Iteration 5900, Loss: 1673017.744404877\n",
      "Iteration 6000, Loss: 1672671.3070009048\n",
      "Iteration 6100, Loss: 1672354.8790701302\n",
      "Iteration 6200, Loss: 1672065.861099708\n",
      "Iteration 6300, Loss: 1671801.8787545892\n",
      "Iteration 6400, Loss: 1671560.763371928\n",
      "Iteration 6500, Loss: 1671340.5341451217\n",
      "Iteration 6600, Loss: 1671139.3818511276\n",
      "Iteration 6700, Loss: 1670955.6539873653\n",
      "Iteration 6800, Loss: 1670787.8411961095\n",
      "Iteration 6900, Loss: 1670634.5648648425\n",
      "Iteration 7000, Loss: 1670494.5658007013\n",
      "Iteration 7100, Loss: 1670366.6938859809\n",
      "Iteration 7200, Loss: 1670249.8986297091\n",
      "Iteration 7300, Loss: 1670143.220537671\n",
      "Iteration 7400, Loss: 1670045.7832299909\n",
      "Iteration 7500, Loss: 1669956.786241514\n",
      "Iteration 7600, Loss: 1669875.4984458366\n",
      "Iteration 7700, Loss: 1669801.252048974\n",
      "Iteration 7800, Loss: 1669733.4371033087\n",
      "Iteration 7900, Loss: 1669671.4964967621\n",
      "Iteration 8000, Loss: 1669614.921376017\n",
      "Iteration 8100, Loss: 1669563.2469661944\n",
      "Iteration 8200, Loss: 1669516.0487526448\n",
      "Iteration 8300, Loss: 1669472.9389934833\n",
      "Iteration 8400, Loss: 1669433.5635342193\n",
      "Iteration 8500, Loss: 1669397.598898313\n",
      "Iteration 8600, Loss: 1669364.7496297571\n",
      "Iteration 8700, Loss: 1669334.745865852\n",
      "Iteration 8800, Loss: 1669307.3411202356\n",
      "Iteration 8900, Loss: 1669282.3102579517\n",
      "Iteration 9000, Loss: 1669259.4476459266\n",
      "Iteration 9100, Loss: 1669238.5654636556\n",
      "Iteration 9200, Loss: 1669219.492160224\n",
      "Iteration 9300, Loss: 1669202.0710449843\n",
      "Iteration 9400, Loss: 1669186.159000315\n",
      "Iteration 9500, Loss: 1669171.6253058831\n",
      "Iteration 9600, Loss: 1669158.3505647532\n",
      "Iteration 9700, Loss: 1669146.2257225183\n",
      "Iteration 9800, Loss: 1669135.1511714004\n",
      "Iteration 9900, Loss: 1669125.035931954\n",
      "Iteration 10000, Loss: 1669115.796905652\n",
      "Iteration 10100, Loss: 1669107.358192217\n",
      "Iteration 10200, Loss: 1669099.6504660884\n",
      "Iteration 10300, Loss: 1669092.610406898\n",
      "Iteration 10400, Loss: 1669086.1801792819\n",
      "Iteration 10500, Loss: 1669080.3069577543\n",
      "Iteration 10600, Loss: 1669074.9424927349\n",
      "Iteration 10700, Loss: 1669070.042714171\n",
      "Iteration 10800, Loss: 1669065.5673694927\n",
      "Iteration 10900, Loss: 1669061.4796929315\n",
      "Iteration 11000, Loss: 1669057.7461034837\n",
      "Iteration 11100, Loss: 1669054.3359290338\n",
      "Iteration 11200, Loss: 1669051.2211543813\n",
      "Iteration 11300, Loss: 1669048.3761910866\n",
      "Iteration 11400, Loss: 1669045.7776672605\n",
      "Iteration 11500, Loss: 1669043.4042355584\n",
      "Iteration 11600, Loss: 1669041.2363978089\n",
      "Iteration 11700, Loss: 1669039.2563448306\n",
      "Iteration 11800, Loss: 1669037.44781013\n",
      "Iteration 11900, Loss: 1669035.7959362662\n",
      "Iteration 12000, Loss: 1669034.2871527956\n",
      "Iteration 12100, Loss: 1669032.909064789\n",
      "Iteration 12200, Loss: 1669031.650351003\n",
      "Iteration 12300, Loss: 1669030.5006708754\n",
      "Iteration 12400, Loss: 1669029.4505795757\n",
      "Iteration 12500, Loss: 1669028.4914504134\n",
      "Iteration 12600, Loss: 1669027.6154039684\n",
      "Iteration 12700, Loss: 1669026.8152433601\n",
      "Iteration 12800, Loss: 1669026.084395124\n",
      "Iteration 12900, Loss: 1669025.4168552095\n",
      "Iteration 13000, Loss: 1669024.807139655\n",
      "Iteration 13100, Loss: 1669024.250239537\n",
      "Iteration 13200, Loss: 1669023.7415798195\n",
      "Iteration 13300, Loss: 1669023.2769817708\n",
      "Iteration 13400, Loss: 1669022.8526286334\n",
      "Iteration 13500, Loss: 1669022.46503427\n",
      "Iteration 13600, Loss: 1669022.1110145217\n",
      "Iteration 13700, Loss: 1669021.787661053\n",
      "Iteration 13800, Loss: 1669021.4923174558\n",
      "Iteration 13900, Loss: 1669021.2225574292\n",
      "Iteration 14000, Loss: 1669020.9761648457\n",
      "Iteration 14100, Loss: 1669020.7511155454\n",
      "Iteration 14200, Loss: 1669020.5455607073\n",
      "Iteration 14300, Loss: 1669020.3578116617\n",
      "Iteration 14400, Loss: 1669020.1863260155\n",
      "Iteration 14500, Loss: 1669020.0296949833\n",
      "Iteration 14600, Loss: 1669019.886631813\n",
      "Iteration 14700, Loss: 1669019.7559612137\n",
      "Iteration 14800, Loss: 1669019.636609704\n",
      "Iteration 14900, Loss: 1669019.5275967892\n",
      "Iteration 15000, Loss: 1669019.428026908\n",
      "Slope: [937.18973136]; Intercept: 152.91935861712994\n",
      "Mean squared error: 2549.27\n",
      "Coefficient of determination: 0.47\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Success ******\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_train, y_train, X_test, y_test = generate_data()\n",
    "    model = SimpleLinearRegression()\n",
    "    model.fit(X_train,y_train)\n",
    "    predicted = model.predict(X_test)\n",
    "    evaluate(model, X_test, y_test, predicted)\n",
    "    pkl_filename = \"iris_model.pkl\"\n",
    "    with open(pkl_filename, 'wb') as file:\n",
    "        pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad5dccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b54c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
